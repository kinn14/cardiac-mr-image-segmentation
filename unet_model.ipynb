{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "TPU",
    "colab": {
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.7"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2guthgOz-Ssm"
      },
      "source": [
        "# Coursework for Cardiac MR Image Segmentation (2021-2022)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gsnVbP35-Sso"
      },
      "source": [
        "## 1. Load, show, and save images with OpenCV\n",
        "\n",
        "OpenCV is an open-source computer vision library which helps us to manipulate image data. In this section, we will cover:\n",
        "* Loading an image from file with imread()\n",
        "* Displaying the image with matplotlib plt.imshow()\n",
        "* Saving an image with imwrite()\n",
        "\n",
        "For a more comprehensive study of OpenCV, we encourage you to check the official [OpenCV documentation](https://docs.opencv.org/master/index.html)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C7ZvSiY3qW_U"
      },
      "source": [
        "from matplotlib import pyplot as plt\n",
        "def show_image_mask(img, mask, cmap='gray'): # visualisation\n",
        "    fig = plt.figure(figsize=(5,5))\n",
        "    plt.subplot(1, 2, 1)\n",
        "    plt.imshow(img, cmap=cmap)\n",
        "    plt.axis('off')\n",
        "    plt.subplot(1, 2, 2)\n",
        "    plt.imshow(mask, cmap=cmap)\n",
        "    plt.axis('off')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EN5WJ_XG-Sso",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 297
        },
        "outputId": "e2564a2c-dce9-402e-faff-d3753530a82d"
      },
      "source": [
        "import os\n",
        "import cv2 #import OpenCV\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "path = \"/content/drive/MyDrive/Colab Notebooks/CW/data\"\n",
        "train_data_dir = path + '/train'\n",
        "val_data_dir = path + '/val'\n",
        "test_data_dir = path + '/test'\n",
        "\n",
        "image = cv2.imread(os.path.join(train_data_dir,'image','cmr300.png'), cv2.IMREAD_UNCHANGED)\n",
        "mask = cv2.imread(os.path.join(train_data_dir,'mask','cmr300_mask.png'), cv2.IMREAD_UNCHANGED)\n",
        "print(image[0])\n",
        "show_image_mask(image, mask, cmap='gray')\n",
        "plt.pause(1)\n",
        "cv2.imwrite(os.path.join('./','cmr201.png'), mask*85)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "[  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "   0   0   0   0   0   0   0   0   0   0   0   0   0 233 226 221 205 222\n",
            " 156  84 101 187 180 201 138 153  29 143 107 143 102  53   0   0   0   0\n",
            "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "   0   0   0   0   0   0]\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 360x360 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x_XthKeXQTYf"
      },
      "source": [
        "## 1.1 Creating more **images** with Data Augmentation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZKmjb6fGxkJC"
      },
      "source": [
        "#import libraries needed for data augmentation\n",
        "import elasticdeform\n",
        "import random"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cHjxdfa7xkJD"
      },
      "source": [
        "#perform augmentation\n",
        "for t in range(1,10):\n",
        "    for i in range(1,101):\n",
        "        image = cv2.imread(os.path.join(train_data_dir,'image','cmr'+str(i)+'.png'), cv2.IMREAD_UNCHANGED) #reading image\n",
        "        mask = cv2.imread(os.path.join(train_data_dir,'mask','cmr'+str(i)+'_mask.png'), cv2.IMREAD_UNCHANGED) #reading mask\n",
        "        #randomly adding deformations\n",
        "        [image_deformed, mask_deformed] = elasticdeform.deform_random_grid([image, mask],sigma=random.randrange(0,5),order=0,rotate=random.randrange(-270,270),zoom=round(random.uniform(0.8,1.5),2))\n",
        "        cv2.imwrite(os.path.join(train_data_dir,'mask','cmr'+str(1100+i+100*(t-1))+'_mask.png'),mask_deformed) #saving deformed image\n",
        "        cv2.imwrite(os.path.join(train_data_dir,'image','cmr'+str(1100+i+100*(t-1))+'.png'),image_deformed) #saving deformed mask"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7UDvsGZnYfHS"
      },
      "source": [
        "Note: You will no doubt notice that the mask images appear to be completely black with no sign of any segmentations. This is because the max intensity of pixels in an 8-bit png image is 255 and your image viewer software only sees 255 as white. For those values close to zero, you will only see dark values. This is the case for our masks as the background, the right ventricle, the myocardium, and the left ventricle in each image are 0, 1, 2, and 3, respectively. All of which are close to zero. If we multiply the original mask by 85 and save the result to the directory where this code is, we can see the heart indeed shows up. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hULAX3WH-Sss"
      },
      "source": [
        "## 2 Define a segmentation model with Pytorch\n",
        "* Define a Segmentation Model\n",
        "* Define a DataLoader that inputs images to the Model\n",
        "* Define training parameters and train the model\n",
        "* Test the trained model with a new input image"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IrKFgoZvUbeg"
      },
      "source": [
        "### 2.1 Define a DataLoader"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XYrD95T8qz8T"
      },
      "source": [
        "import torch\n",
        "import torch.utils.data as data\n",
        "import os\n",
        "from glob import glob"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7dqufK1jxkJH"
      },
      "source": [
        "#define dataloaders\n",
        "class TrainDataset(data.Dataset):\n",
        "    def __init__(self, root=''):\n",
        "        super(TrainDataset, self).__init__()\n",
        "        self.img_files = glob(os.path.join(root,'image','*.png'))\n",
        "        self.mask_files = []\n",
        "        for img_path in self.img_files:\n",
        "            basename = os.path.basename(img_path)\n",
        "            self.mask_files.append(os.path.join(root,'mask',basename[:-4]+'_mask.png'))\n",
        "            \n",
        "\n",
        "    def __getitem__(self, index):\n",
        "            img_path = self.img_files[index]\n",
        "            mask_path = self.mask_files[index]\n",
        "            data = cv2.imread(img_path, cv2.IMREAD_UNCHANGED)\n",
        "            label = cv2.imread(mask_path, cv2.IMREAD_UNCHANGED)\n",
        "            return torch.from_numpy(data).float(), torch.from_numpy(label).float()\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.img_files)\n",
        "\n",
        "class ValDataset(data.Dataset):\n",
        "    def __init__(self, root=''):\n",
        "        super(ValDataset, self).__init__()\n",
        "        self.img_files = glob(os.path.join(root,'image','*.png'))\n",
        "        self.mask_files = []\n",
        "        for img_path in self.img_files:\n",
        "            basename = os.path.basename(img_path)\n",
        "            self.mask_files.append(os.path.join(root,'mask',basename[:-4]+'_mask.png'))\n",
        "            \n",
        "    def __getitem__(self, index):\n",
        "            img_path = self.img_files[index]\n",
        "            mask_path = self.mask_files[index]\n",
        "            data = cv2.imread(img_path, cv2.IMREAD_UNCHANGED)\n",
        "            label = cv2.imread(mask_path, cv2.IMREAD_UNCHANGED)\n",
        "            return torch.from_numpy(data).float(), torch.from_numpy(label).float()\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.img_files)\n",
        "\n",
        "class TestDataset(data.Dataset):\n",
        "    def __init__(self, root=''):\n",
        "        super(TestDataset, self).__init__()\n",
        "        self.img_files = glob(os.path.join(root,'image','*.png'))\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "            img_path = self.img_files[index]\n",
        "            data = cv2.imread(img_path, cv2.IMREAD_UNCHANGED)\n",
        "            return torch.from_numpy(data).float(), img_path\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.img_files)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ghect8gIR9RB",
        "outputId": "b3d35f35-4338-4365-8d15-c38313291ce2"
      },
      "source": [
        "#checking if gpu is available\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"The current device is {device}\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The current device is cpu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "82UAfnwSUgc_"
      },
      "source": [
        "### 2.2 Define a Segmenatation Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gEIkCqdfYnIn"
      },
      "source": [
        "Defining a Unet Model with 24 layers and depth of 4 for Segmentation."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-W6532hFXa_g"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "\n",
        "#Defining model based on UNet architecture\n",
        "class UNET2(nn.Module): \n",
        "    def __init__(self):\n",
        "        super(UNET2, self).__init__()\n",
        "        initial_conv_value = 64\n",
        "        #first conv layer takes 1 input channel and outputs activation map with 64 channels\n",
        "        self.conv_input_layer0 = nn.Conv2d(1, inz, 3, padding=1)\n",
        "        self.conv_layer0_layer0 = nn.Conv2d(64 ,64, 3, padding=1)\n",
        "        self.pool = nn.MaxPool2d(2,2)\n",
        "\n",
        "        #we define all our non-linear activations to use ReLu\n",
        "        self.nonlinear = nn.ReLU()\n",
        "\n",
        "        #define remaining conv layers\n",
        "        self.conv_layer0_layer1 = nn.Conv2d(64, 128, 3, padding=1)\n",
        "        self.conv_layer1_layer1 = nn.Conv2d(128, 128, 3, padding=1)\n",
        "        self.conv_layer1_layer2 = nn.Conv2d(128, 256, 3, padding=1)\n",
        "        self.conv_layer2_layer2 = nn.Conv2d(256, 256, 3, padding=1)\n",
        "        self.conv_layer2_layer3 = nn.Conv2d(256, 512, 3, padding=1)\n",
        "        self.conv_layer3_layer3 = nn.Conv2d(512, 512, 3, padding=1)\n",
        "        self.conv_layer3_layer4 = nn.Conv2d(512, 1024, 3, padding=1)\n",
        "        self.conv_layer4_layer4 = nn.Conv2d(1024, 1024, 3, padding=1)\n",
        "\n",
        "        #define deconv layers to get original input size back for output\n",
        "        self.deconv_layer4_layer3 = nn.ConvTranspose2d(1024, 512, 2, stride=2)\n",
        "        self.conv_layer4_layer3 = nn.Conv2d(1024, 512, 3, padding=1)\n",
        "        \n",
        "        self.deconv_layer3_layer2 = nn.ConvTranspose2d(512, 256, 2, stride=2)\n",
        "        self.conv_layer3_layer2 = nn.Conv2d(512, 256, 3, padding=1)\n",
        "        \n",
        "        self.deconv_layer2_layer1 = nn.ConvTranspose2d(256, 128, 2, stride=2)\n",
        "        self.conv_layer2_layer1 = nn.Conv2d(256, 128, 3, padding=1)\n",
        "        \n",
        "        self.deconv_layer1_layer0 = nn.ConvTranspose2d(128, 64, 2, stride=2)\n",
        "        self.conv_layer1_layer0 = nn.Conv2d(128, 64, 3, padding=1)\n",
        "        \n",
        "        #final output layer outputs an image with 4 channels\n",
        "        self.conv_layer0_output = nn.Conv2d(64, 4, 1)\n",
        "        \n",
        "    def forward(self, x):\n",
        "        \n",
        "        #define forward propagation steps\n",
        "        x_0 = self.nonlinear(self.conv_layer0_layer0(self.nonlinear(self.conv_input_layer0(x))))\n",
        "        x_1 = self.pool(x_0)\n",
        "        \n",
        "        x_1 = self.nonlinear(self.conv_layer1_layer1(self.nonlinear(self.conv_layer0_layer1(x_1))))\n",
        "        x_2 = self.pool(x_1)\n",
        "        \n",
        "        x_2 = self.nonlinear(self.conv_layer2_layer2(self.nonlinear(self.conv_layer1_layer2(x_2))))\n",
        "        x_3 = self.pool(x_2)\n",
        "        \n",
        "        x_3 = self.nonlinear(self.conv_layer3_layer3(self.nonlinear(self.conv_layer2_layer3(x_3))))\n",
        "        x_4 = self.pool(x_3)\n",
        "        \n",
        "        x_4 = self.nonlinear(self.conv_layer4_layer4(self.nonlinear(self.conv_layer3_layer4(x_4))))\n",
        "        \n",
        "        x_3_up = self.deconv_layer4_layer3(x_4)\n",
        "        x_3 = torch.cat((x_3, x_3_up), 1)\n",
        "        x_3 = self.nonlinear(self.conv_layer3_layer3(self.nonlinear(self.conv_layer4_layer3(x_3))))\n",
        "        \n",
        "        x_2_up = self.deconv_layer3_layer2(x_3)\n",
        "        x_2 = torch.cat((x_2, x_2_up), 1)\n",
        "        x_2 = self.nonlinear(self.conv_layer2_layer2(self.nonlinear(self.conv_layer3_layer2(x_2))))\n",
        "        \n",
        "        x_1_up = self.deconv_layer2_layer1(x_2)\n",
        "        x_1 = torch.cat((x_1, x_1_up), 1)\n",
        "        x_1 = self.nonlinear(self.conv_layer1_layer1(self.nonlinear(self.conv_layer2_layer1(x_1))))\n",
        "        \n",
        "        x_0_up = self.deconv_layer1_layer0(x_1)\n",
        "        x_0 = torch.cat((x_0, x_0_up), 1)\n",
        "        x_0 = self.nonlinear(self.conv_layer0_layer0(self.nonlinear(self.conv_layer1_layer0(x_0))))\n",
        "            \n",
        "        x = self.conv_layer0_output(x_0)\n",
        "     \n",
        "        return x\n",
        "\n",
        "#instantiate model and transfer to gpu (if available)\n",
        "model = UNET2().to(device) "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nRdPFTa9a34J"
      },
      "source": [
        "### 2.3 Define a Loss function and optimizer\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QRjOZGXRbUFT"
      },
      "source": [
        "import torch.optim as optim\n",
        "\n",
        "#hyperparammeters for optimiser\n",
        "learning_rate = 0.0002\n",
        "betas = (0.9, 0.99)\n",
        "wgts = [0.1, 0.3, 0.3, 0.3]\n",
        "\n",
        "#weights for loss function\n",
        "weights = torch.tensor(wgts).to(device)\n",
        "\n",
        "#define loss and optimiser\n",
        "Loss = nn.CrossEntropyLoss(weight=weights)\n",
        "Loss.requires_grad_(True)\n",
        "optimizer = optim.Adam(model.parameters(), lr=learning_rate, betas=betas)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "grDz3fR1qW_V"
      },
      "source": [
        "### 2.4 Training\n",
        "\n",
        "As most of you will use CPUs to train the model, expect your models to take **30 minutes to train if not longer depending on network architecture**. To save time, you should not be using all training data until your model is well developed. If you are running your model on a GPU training should be significantly faster. During the training process, you may want to save the checkpoints as follows:\n",
        "\n",
        "```\n",
        "# Saving checkpoints for validation/testing\n",
        "torch.save(model.state_dict(), path)\n",
        "```\n",
        "The saved checkpoints can be used to load at a later date for validation and testing. Here we give some example code for training a model. Note that you need to specify the max iterations you want to train the model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sapBzJmfOTMj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dede7279-279c-4d26-c8e1-7a1669fd994f"
      },
      "source": [
        "from torch.utils.data import DataLoader\n",
        "import numpy as np\n",
        "\n",
        "# get the model\n",
        "model = model.to(device)\n",
        "\n",
        "# define parameters for training \n",
        "num_workers = 4\n",
        "batch_size = 25\n",
        "max_epochs = 50\n",
        "\n",
        "# seed generator for worker \n",
        "def seed_worker(worker_id):\n",
        "    worker_seed = torch.initial_seed() % 2**32\n",
        "    np.random.seed(worker_seed)\n",
        "    \n",
        "g = torch.Generator()\n",
        "g.manual_seed(0)\n",
        "\n",
        "# add 2 params in the fn worker_init_fn=seed_worker, generator=g\n",
        "train_set = TrainDataset(train_data_dir)\n",
        "training_data_loader = DataLoader(dataset=train_set, num_workers=num_workers, batch_size=batch_size, worker_init_fn=seed_worker, generator=g, shuffle=True)\n",
        "\n",
        "# define vars to store epoch stats\n",
        "epochs = [x for x in range(1, max_epochs+1)]\n",
        "running_loss = 0.0\n",
        "final_loss = []\n",
        "\n",
        " \n",
        "for epoch in range(max_epochs):\n",
        "    \n",
        "    # Fetch images and labels. \n",
        "    for iteration, sample in enumerate(training_data_loader):\n",
        "        img, mask = sample\n",
        "\n",
        "        model.train() #put model in train mode\n",
        "        optimizer.zero_grad() #set gradients to zero to avoid accumulating them\n",
        "        \n",
        "        #Get image in correct format and transfer to gpu (if available)\n",
        "        image_for_model = torch.unsqueeze(img, 1).to(device)\n",
        "\n",
        "        #pass image through network and get output using forward propagation\n",
        "        predicted_mask = model(image_for_model).to(device)\n",
        "        \n",
        "        #calculate the loss usinde loss function defined above\n",
        "        loss = Loss(predicted_mask, mask.long().to(device))\n",
        "   \n",
        "        #Do backward proagation and step through with optimiser to update network parameters\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        \n",
        "        #calculate running loss\n",
        "        running_loss += loss.item()\n",
        "   \n",
        "        value, predicted_mask_image = torch.max(predicted_mask, 1)\n",
        "    print('[%d, %5d] loss: %.3f' %(epoch + 1, iteration + 1, running_loss/(iteration+1))) #print loss for epoch\n",
        "    final_loss.append(running_loss) #computing final loss for plotting\n",
        "    running_loss = 0.0\n",
        "\n",
        "#visualise final predicted mask passed through network     \n",
        "show_image_mask(img[0,...].squeeze().cpu(), mask[0,...].squeeze().cpu())\n",
        "show_image_mask(img[0,...].squeeze().cpu(), predicted_mask_image[0,...].squeeze().cpu())\n",
        "plt.pause(1)\n",
        "\n",
        "#save the model\n",
        "torch.save(model.state_dict(), \"./tensor.pth\")\n",
        "\n",
        "# plot loss graph\n",
        "plt.plot(epochs,final_loss)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tCZP-xof-Sst"
      },
      "source": [
        "### 2.5 Validating and Testing\n",
        "\n",
        "Using the Categorical Dice Loss in order to check the validation set accuracy."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y1BOHUDwKdwu"
      },
      "source": [
        "def categorical_dice(mask1, mask2, label_class=1):\n",
        "    \"\"\"\n",
        "    Dice score of a specified class between two volumes of label masks.\n",
        "    (classes are encoded but by label class number not one-hot )\n",
        "    Note: stacks of 2D slices are considered volumes.\n",
        "\n",
        "    Args:\n",
        "        mask1: N label masks, numpy array shaped (H, W, N)\n",
        "        mask2: N label masks, numpy array shaped (H, W, N)\n",
        "        label_class: the class over which to calculate dice scores\n",
        "\n",
        "    Returns:\n",
        "        volume_dice\n",
        "    \"\"\"\n",
        "    mask1_pos = (mask1 == label_class).astype(np.float32)\n",
        "    mask2_pos = (mask2 == label_class).astype(np.float32)\n",
        "    dice = 2 * np.sum(mask1_pos * mask2_pos) / (np.sum(mask1_pos) + np.sum(mask2_pos))\n",
        "    return dice"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8tyklpG5VJPS"
      },
      "source": [
        "#Validating model on Validation Set\n",
        "\n",
        "#set parameters\n",
        "num_workers = 4\n",
        "batch_size = 1\n",
        "\n",
        "#load validation images\n",
        "val_set = ValDataset(val_data_dir)\n",
        "val_data_loader = DataLoader(dataset=val_set, num_workers=num_workers,batch_size=batch_size, shuffle=False)\n",
        "\n",
        "#load saved model and set to eval model\n",
        "model = UNET2()\n",
        "model.load_state_dict(torch.load(\"./tensor.pth\"))\n",
        "model.eval()\n",
        "\n",
        "#loss variables\n",
        "avg_loss = 0\n",
        "dice_loss = 0\n",
        "total_dice_loss = 0\n",
        "\n",
        "#var to keep track of number of iterations\n",
        "i = 0\n",
        "\n",
        "with torch.no_grad():\n",
        "    for iteration, sample in enumerate(val_data_loader):\n",
        "\n",
        "        dice_loss = 0\n",
        "        img, mask = sample\n",
        "        img = img.unsqueeze(1)\n",
        "\n",
        "        #get predicted mask using model\n",
        "        outputs = model(img)\n",
        "\n",
        "        #calculate estimated categorical dice loss for each class and average\n",
        "        _, predicted = torch.max(outputs, 1)\n",
        "        dice_loss += categorical_dice(mask.numpy(), predicted.numpy(), 1)\n",
        "        dice_loss += categorical_dice(mask.numpy(), predicted.numpy(), 2)\n",
        "        dice_loss += categorical_dice(mask.numpy(), predicted.numpy(), 3)\n",
        "        dice_loss /= 3\n",
        "        total_dice_loss += dice_loss\n",
        "\n",
        "        #visualise predicted and true mask\n",
        "        # show_image_mask(img[0,...].squeeze(), mask[0,...].squeeze(), cmap='gray')\n",
        "        # show_image_mask(img[0,...].squeeze(), predicted[0,...].squeeze(), cmap='gray')    \n",
        "        \n",
        "        i += 1\n",
        "\n",
        "#calculate dice loss average over all validation examples\n",
        "total_dice_loss = total_dice_loss / i\n",
        "\n",
        "#print loss\n",
        "print(\"Categorical Dice Loss: \", total_dice_loss)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LVS22lrjqW_V"
      },
      "source": [
        "# Predicting the mask images for test data set and storing in a directory\n",
        "\n",
        "# parameters\n",
        "num_workers = 0\n",
        "batch_size = 1\n",
        "\n",
        "# load test data\n",
        "test_set = TestDataset(test_data_dir)\n",
        "test_data_loader = DataLoader(dataset=test_set, num_workers=num_workers,batch_size=batch_size, shuffle=False)\n",
        "\n",
        "# load saved model\n",
        "model = UNET2().to(device)\n",
        "model.load_state_dict(torch.load(\"./tensor.pth\"))\n",
        "model.eval()\n",
        "\n",
        "for iteration, (sample, path_name) in enumerate(test_data_loader):\n",
        "\n",
        "    img_number = str(path_name).split(\"cmr\")[-1].split(\".\")[0]\n",
        "\n",
        "    img = sample\n",
        "    img = img.unsqueeze(1).to(device)\n",
        "\n",
        "    #get predicted mask\n",
        "    outputs = model(img).to(device)\n",
        "    _, predicted = torch.max(outputs, 1)\n",
        "\n",
        "    #get predicted mask to save as numpy array\n",
        "    npi = predicted[0].cpu().numpy()\n",
        "  \n",
        "    #define name of png file to save predicted mask as\n",
        "    file_name = 'cmr' + img_number + '_mask.png'\n",
        "  \n",
        "    #save png file\n",
        "    cv2.imwrite((test_data_dir + \"/mask/\" + file_name), npi)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wsycVbIuUov3"
      },
      "source": [
        "## 3 Evaluation\n",
        "\n",
        "As we will automatically evaluate your predicted test makes on Kaggle, in this section we expect you to learn:\n",
        "* what is the Dice score used on Kaggle to measure your models performance\n",
        "* how to submit your predicted masks to Kaggle"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NicQyj47jsD1"
      },
      "source": [
        "### 3.1 Dice Score\n",
        "\n",
        "To evaluate the quality of the predicted masks, the Dice score is adopted. Dice score on two masks A and B is defined as the intersection ratio between the overlap area and the average area of two masks. A higher Dice suggests a better registration.\n",
        "\n",
        "$Dice (A, B)= \\frac{2|A \\cap B|}{|A| + |B|} $\n",
        "\n",
        "However, in our coursework, we have three labels in each mask, we will compute the Dice score for each label and then average the three of them as the final score. Below we have given you `categorical_dice` for free so you can test your results before submission to Kaggle."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JZcsrwmVjy5k"
      },
      "source": [
        "### 3.2 Submission\n",
        "\n",
        "Kaggle requires your submission to be in a specific CSV format. To help ensure your submissions are in the correct format, we have provided some helper functions to do this for you. For those interested, the png images are run-length encoded and saved in a CSV to the specifications required by our competition.\n",
        "\n",
        "It is sufficient to use this helper function. To do so, save your 80 predicted masks into a directory. ONLY the 80 predicted masks should be in this directory. Call the submission_converter function with the first argument as the directory containing your masks, and the second the directory in which you wish to save your submission."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uHDVbgu0qW_V"
      },
      "source": [
        "import numpy as np\n",
        "import os\n",
        "import cv2\n",
        "\n",
        "def rle_encoding(x):\n",
        "    '''\n",
        "    *** Credit to https://www.kaggle.com/rakhlin/fast-run-length-encoding-python ***\n",
        "    x: numpy array of shape (height, width), 1 - mask, 0 - background\n",
        "    Returns run length as list\n",
        "    '''\n",
        "    dots = np.where(x.T.flatten() == 1)[0]\n",
        "    run_lengths = []\n",
        "    prev = -2\n",
        "    for b in dots:\n",
        "        if (b > prev + 1): run_lengths.extend((b + 1, 0))\n",
        "        run_lengths[-1] += 1\n",
        "        prev = b\n",
        "    return run_lengths\n",
        "\n",
        "\n",
        "def submission_converter(mask_directory, path_to_save):\n",
        "    writer = open(os.path.join(path_to_save, \"submission.csv\"), 'w')\n",
        "    writer.write('id,encoding\\n')\n",
        "\n",
        "    files = os.listdir(mask_directory)\n",
        "\n",
        "    for file in files:\n",
        "        name = file[:-4]\n",
        "        mask = cv2.imread(os.path.join(mask_directory, file), cv2.IMREAD_UNCHANGED)\n",
        "\n",
        "        mask1 = (mask == 1)\n",
        "        mask2 = (mask == 2)\n",
        "        mask3 = (mask == 3)\n",
        "\n",
        "        encoded_mask1 = rle_encoding(mask1)\n",
        "        encoded_mask1 = ' '.join(str(e) for e in encoded_mask1)\n",
        "        encoded_mask2 = rle_encoding(mask2)\n",
        "        encoded_mask2 = ' '.join(str(e) for e in encoded_mask2)\n",
        "        encoded_mask3 = rle_encoding(mask3)\n",
        "        encoded_mask3 = ' '.join(str(e) for e in encoded_mask3)\n",
        "\n",
        "        writer.write(name + '1,' + encoded_mask1 + \"\\n\")\n",
        "        writer.write(name + '2,' + encoded_mask2 + \"\\n\")\n",
        "        writer.write(name + '3,' + encoded_mask3 + \"\\n\")\n",
        "\n",
        "    writer.close()\n",
        "\n",
        "#call function to submit our images\n",
        "submission_converter(test_data_dir + '/mask', test_data_dir)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NjlJoBLpZLKc"
      },
      "source": [
        "#Introduction \n",
        "\n",
        "The task we are attempting is to segment cardiovascular magnetic resonance (CMR) images into four regions: the background, left ventricle, myocardium and right ventricle. Well be using semantic segmentation to do this, which assigns a label to all the pixels in an image, where pixels that share characteristics will have the same label. To perform this task we will be constructing a convolutional neural network (CNN). The dataset we are using is a modified version from the ACDC<sub>1</sub> challenge. It contains 200 CMR images and corresponding segmented masks. The dataset will be split 50%, 10% and 40% for training, validation and testing respectively. During this experiment, we will to explore a variety of CNN architectures and hyperparameters in order to implement a neural network which can accurately segment cardiovascular MR images.  \n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GZbUWwiWZPbA"
      },
      "source": [
        "#Implementation  <sub>2</sub>\n",
        "\n",
        "The model currently employed is the U-net architecture. U-net is a fully convolutional neural network with a symmetric architecture, hence the U-shape. It consists of two major parts  the left part is called contracting path, which is constituted by the general convolutional process; the right part is an expansive path, which is constituted by transposed 2D convolutional layers. In the contracting path, the network learns important features and in the expansive path tries to enhance the resolution of the image to bring it to the original size of the image. In the contracting path U-net uses max pooling layers to decrease the size of the image and in the expansive path it replaces the max pooling layers with deconvolution layers to amplify small features. The same idea is also adopted in DeepLab to enhance the resolution <sub>3</sub>. The fully connected layers are also removed so that small changes in regions where the main object occurs does not affect the prediction.\n",
        "The main reason for going forward with this architecture was due to the fact that U-net has a major role in medical image segmentation, which coincides with our task. The only difference in our proposed architecture is that we add padding in every convolution layer unlike the original network where the borders are cropped.\n",
        "In our network we apply two 3 x 3 convolutions (padding =1) each succeeded by a ReLU. Then we apply 2 x 2 maxpooling operation with a stride of 2. After every maxpooling the feature channels are doubled. The above combination is applied 4 times until we reach an image of dimensions 6 x 6 x 1024 after which we apply a convolution + ReLU twice. In the expansive path we upsample the data and we concatenated with the corresponding feature map. Then again, we apply two 3 x 3 convolutions each succeeded by ReLU, hence obtaining a symmetric architecture. The feature maps are halved every time we upsample the data. The expansive step is applied until we reach a feature map of size 96 x 96 x 64. Finally, we apply a 1 x 1 convolution that maps each feature vector to the 4 classes. The below figure illustrates our architecture:\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "<figure>\n",
        "<center>\n",
        "<img src='https://drive.google.com/uc?id=12jyIfYwbRnS2XlMeLLDTSLDCVM-0p2H2' />\n",
        "</figure>\n",
        "\n",
        "Initially we research a variety of CNNs, looking at both academic papers and online articles, in order to get a broad understanding of them and how their different architectures may impact the result of the model. From our research and implementations, we found that architectures which have many layers such as DeconvNet and Deeplab both produced bad results primarily due to the fact we have a small dataset, which leads to overfitting while training the model. Despite initial concerns about lack of computational power, we found that SegNet produced good results. However, we decided to use UNet as it produced the best results from our initial experiments, and our research found that it is the best architecture to use when working with low volumes of data <sub>4</sub>.\n",
        "\n",
        "##Loss functions considered:\n",
        "The main idea of all loss functions and optimizers is to determine the point where the function is the minimum. However, most algorithms take the assumption that the function is convex. Below we detail the different loss functions we considered using for training. \n",
        "###Lovasz Softmax:\n",
        "Lovasz Softmax is based on the idea of Lovasz hinge and this method assumes that we are dealing with a convex function. As a result, if the function is not convex, we would not be able to find the global minima and may get stuck in a local minimum <sub>5</sub>.\n",
        "###Dice Loss:\n",
        "Dice loss uses the Dice coefficent to calculate the overlap between two images. It is computed using the following formula:\n",
        "\n",
        "$Dice (A, B)= \\frac{2|A \\cap B|}{|A| + |B|} $\n",
        "\n",
        "*where A and B are images.* <sub>6</sub>\n",
        "###Focal Loss:\n",
        "Focal loss builds on standard cross-entropy loss, such that loss can be calculated using:\n",
        "$FL(p$<sub>t</sub>$)=(1-p)$<sup>$$</sup>$log(p$<sub>t</sub>$)$\n",
        "\n",
        "Adding the factor $(1-p)$<sup>$$</sup> reduces the weighting of well classified examples. This means that misclassfied examples have a greater impact of the overall loss.<sub>7</sub>\n",
        "\n",
        "###Boundary Loss:\n",
        "Boundary loss uses distance metric on space contours rather than segmentation regions that other losses such as Dice or Cross-entropy use. Using integrals over the interface of regions enables it to work well on highly unbalanced problems. <sub>8</sub>\n",
        "\n",
        "##Loss function implemented:\n",
        "###Cross-Entropy:\n",
        "Cross entropy is based on the concept of entropy in information theory and is different from KL Divergence (i.e Kullback Leibler Divergence), which calculates the relative entropy between two probability distributions. Cross-entropy loss deals with finding the total entropy between the distributions. This is represented by taking the negative log-likelihood where the increased negative value indicates a higher loss. We have intuitively understood that in order to really evaluate which classes (3 channels) of the image data are correctly or not correctly predicted, a loss function that deals with categorical data would be the best choice. <sub>9</sub>\n",
        "\n",
        "##Optimizers considered:   <sub>10</sub>\n",
        "\n",
        "###Stochastic Gradient Descent:\n",
        "SGD randomly picks one data point from the whole data set at each iteration to reduce the computations enormously.\n",
        "It is also common to sample a small number of data points instead of just one point at each step and that is called mini-batch gradient descent. Mini-batch tries to strike a balance between the goodness of gradient descent and speed of SGD.\n",
        "\n",
        "###AdaGrad:\n",
        "Adagrad is an adaptive learning rate method. In Adagrad we adapt the learning rate to the parameters. We perform larger updates for infrequent parameters and smaller updates for frequent parameters. It is well suited when we have sparse data as in large scale neural networks. In Adagrad, we use different learning rates for every set of parameters for each time step.\n",
        "\n",
        "##Optimizer implemented : <sub>10</sub>\n",
        "###Adam:\n",
        "Adam is another method that calculates the individual adaptive learning rate for each parameter from estimates of first and second moments of the gradients.It also reduces the radically diminishing learning rates of Adagrad. Adam can be viewed as a combination of Adagrad, which works well on sparse gradients and RMSprop which works well in online and nonstationary settings. It implements the exponential moving average of the gradients to scale the learning rate instead of a simple average as in Adagrad. It keeps an exponentially decaying average of past gradients. Adam is computationally efficient and has very little memory requirement. Adam optimizer is one of the most popular gradient descent optimization algorithms.\n",
        "Hyper-parameters 1, 2  [0, 1] control the exponential decay rates of these first and second moments.\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lkGrm8PijZuM"
      },
      "source": [
        "#Experiment\n",
        "\n",
        "##Data augmentation:\n",
        "One of our biggest concerns while constructing this model was the limited dataset as it can be difficult to get high accuracy. When using the original 100 MR images for training our best result has a loss of 0.008 when training the model and validation results around 0.857. In order to improve the results of the model, we used data augmentation. Initially, we had just rotated the images, but later found there we could manipulate the images in other ways. For example, using a library called elastic deform we were able to create images similar to the original dataset by changing the standard deviation parameter. The aim of only going down to 0.8 with zooming was to try and decrease the overall prevalence of the background class, as the other classes are mostly centered. We were able to create 1900 extra images through randomised zooming, sigma and rotation of 0.8-1.5, 0-5 and -270 to 270 degrees respectively. Through data augmentation, we were able to increase validation accuracy by 0.02 overall. \n",
        "\n",
        "\n",
        "##Loss functions:\n",
        "All of the experiments regarding loss functions and optimizers were conducted on the original dataset provided.\n",
        "\n",
        "###Dice Loss, Boundary Loss and Focal Loss:\n",
        "\n",
        "Dice, Boundary and Focal loss all required vectors with a single channel, whereas our model had 4 output channels. We attempted to expand it to a multiclass problem for Dice loss, where we computed Dice per-channel and then averaged over channels and batch to get the mean Dice per-class. This approach didnt seem to entirely fit into our data as we want to see the overall performance of the channels rather than see an averaged version of the channel outputs. Therefore, we concluded that these loss functions wouldn't work for our model. \n",
        "\n",
        "However, we still chose to use categorical dice loss as a metric to evaluate the validation loss as that is the metric used for testing. \n",
        "\n",
        "###Lovasz Softmax:\n",
        "The loss values obtained in using Lovasz Softmax for our model is as given below:\n",
        "\n",
        "Epoch | Training Loss | Validation loss\n",
        "---| --- | ---\n",
        "100 | 0.269 | 0.77\n",
        "150 | 7.224|0.1\n",
        "\n",
        "Although Lovasz Softmax worked well for 100 epochs, it eventually ended up giving a higher training loss when run for 150 epochs. Although this has given a smaller validation loss, this may simply be that the validation set may be an ideal reflection of the training set. This may also indicate that the loss value is stuck at a local minimum instead of a global minima as we cannot assure that the function that best fits our model is convex. Taking these points into consideration, Lovasz Softmax did not seem to be a reliable loss function for our dataset and model.\n",
        "\n",
        "\n",
        "###Cross Entropy:\n",
        "\n",
        "The next loss which was experimented with was using weighted and non-weighted cross entropy. The weights were set manually by intuition from data.\n",
        "The training losses for different values of weights with cross entropy with various epochs is as illustrated below:\n",
        "\n",
        "Weight values | Epochs | Training Loss | Validation Loss \n",
        "---| --- | --- | --- |\n",
        "No weights | 100 | 0.036 | 0.80\n",
        "Calculated using frequency of class in training masks | 100 | 0.018 | 0.61\n",
        "[0.1, 0.3, 0.3, 0.3] | 100 | 0.068 | 0.71\n",
        "[0.1, 0.3, 0.3, 0.3] | 130 | 0.163 | 0.735\n",
        "[0.15, 0.3, 0.35, 0.35] |150 | 0.018 | 0.85\n",
        "[0.2, 0.35, 0.35, 0.35] | 150 | 0.058 | 0.81\n",
        "Automatically calculated weights within loss function | 70 | 0.078 | 0.76\n",
        "\n",
        "We gave lower weight to class 0 (the background) in order to prioritise the foreground and ensure we highlight the importance of training the foreground to match as well as possible. \n",
        "Class 1 has slightly lower weight to the other two as the overall average frequency of pixels of class 2 was lower in training masks.\n",
        "Using these manual weights meant that the model took longer to converge but gave a higher validation loss overall. \n",
        "\n",
        "After doing this initial testing, we created more data points and did the same tests, in the end obtaining a slightly different result. We decided to write code that can extract the exact ratio of classes from the data points, and we got results that did fit our own prediction following testing on augmented data, with the optimum weights being: [0.1, 0.3, 0.3, 0.3].\n",
        "\n",
        "###Optimizers:\n",
        "\n",
        "After iterative experiments it was clear that cross entropy would be the loss function to be employed in our model. The next step was to select the optimizer function.\n",
        "\n",
        "<figure>\n",
        "<center>\n",
        "<img src='https://drive.google.com/uc?id=1yqdP-eZL-A4AwohdkDkzBnYjUpCBSJbw' />\n",
        "</figure>\n",
        "\n",
        "\n",
        "The above diagram shows the comparison of various optimizers on MNIST dataset Training a Multilayer Perceptron <sub>11</sub> .\n",
        "Looking at the overall performance and working mechanism of optimizers, the experiment was run by taking the following three optimizers:\n",
        "\n",
        "\n",
        "####Stochastic Gradient Descent:\n",
        "\n",
        "We conducted experiments on various hyperparameters for Stochastic Gradient Descent (SGD), such as the batch size, epochs, learning rate and more. Using a batch size of 10 produced the best results when using a training set of 100 images. A batch size lower than 10 resulted in high fluctuation in the training loss and the model didnt learn quickly enough when the batch size is greater than 10, resulting in a lower validation accuracy. \n",
        "\n",
        "Momentum |Learning Rate | Batch Size | Epochs| Training Loss | Validation Accuracy |\n",
        "---| --- | --- | --- | --- | --- |\n",
        "0.8 | 0.001 | 5 | 200 | 0.265 | 0.482\n",
        "0.8|0.001|5|500|0.099|0.675\n",
        "0.99 |0.001 |10 |400 |0.020 |0.760\n",
        "0.99 | 0.001 |10 | 500 |0.013 |0.788\n",
        "\n",
        "\n",
        "We found that 0.99 momentum gave the best validation accuracy with the highest being 0.788. When the moment was decreased to 0.8 the model produced significantly worse results, with the validation accuracy being 0.494 worse than when the moment was 0.99 and all other parameters consistent. A momentum of 0.999 caused the loss to diverge and often resulted in unreadable results. If the readable results were produced then the loss was extremely large and validation accuracy quite low. For example, 0.999 momentum, 0.001 learning rate, batch size of 10 and a 100 epochs produced a training loss of 0.516 and accuracy of 0.26. Using learning rates larger than 0.001, such as 0.01 and 0.005, also produced unreadable results. \n",
        "\n",
        "Momentum |Learning Rate | Batch Size | Epochs | Training Loss | Validation Accuracy |\n",
        "---| --- | --- | --- | --- | --- | \n",
        "0.999 | 0.001 | 10 | 100 | 0.516 | 0.26\n",
        "0.999 | 0.001 | 10 | 200| NaN | ----\n",
        "0.999 | 0.001 |10 | 300 |NaN |----\n",
        "0.99 |0.001 |10 |500 |0.032 |0.788\n",
        "0.99 |0.01 |10 |200 |NaN |----\n",
        "0.99 | 0.005 | 10 | 20 | NaN |----\n",
        "\n",
        "\n",
        "As this was run on the original training set, we found a large number of iterations needed to be run, with 500 epochs producing the highest validation accuracy. We also found that using Nesterov momentum doesnt improve validation accuracy or training loss. \n",
        "\n",
        "####AdaGrad:\n",
        "\n",
        "Learning Rate | Batch Size | Iterations |Training Loss |Validation Accuracy\n",
        "---| --- | ---| ---| ---\n",
        "0.001|10|200|0.041|0.734|\n",
        "0.001|10|400|0.056|0.753\n",
        "0.001|10|500|0.028|0.740\n",
        "0.001|20|200|0.062|0.664\n",
        "0.001|20|400|0.017|0.698\n",
        "\n",
        "Compared to the other optimizer, AdaGrad produced the worst results, with the highest validation score of 0.753 being achieved when using a learning rate of 0.001, batch size of 10 and 400 epochs. \n",
        "\n",
        "####Adam:\n",
        "\n",
        "When experimenting with the number of epochs we found that until 200 epochs, an increase in epochs resulted in greater accuracy and lower loss. However, after 200 epochs there was a stagnation in that accuracy. We also observed that the larger the learning rate, the faster the model would converge and the training loss didnt fluctuate as much. Despite this, a learning rate above 0.0005 caused a decrease in validation accuracy. \n",
        "\n",
        "Betas|Learning Rate|Batch Size|Epochs|Training Loss|Validation Accuracy\n",
        "---|---|---|---|---|---|\n",
        "(0.9,0.999)|0.0002|10|50|0.051|0.749\n",
        "(0.9,0.999)|0.0002|10|100|0.030|0.784\n",
        "(0.9,0.999)|0.0002|10|200|0.016|0.836\n",
        "(0.99,0.999)|0.0005|10|50|0.116|0.668\n",
        "(0.99,0.999)|0.0005|10|100|0.026|0.801\n",
        "(0.99,0.999)|0.0005|10|200|0.016|0.801\n",
        "(0.9, 0.99)|0.0005|10|30|0.083|0.782\n",
        "(0.9, 0.99)|0.0005|10|50|0.041|0.826\n",
        "(0.9, 0.99)|0.0001|10|30|0.330|0.612\n",
        "(0.9, 0.99)|0.0001|10|50|0.065|0.687\n",
        "\n",
        "We found that when the 2nd beta is 0.999, the model becomes unstable. Initially. the training loss value decreases as expected but then suddenly increases. This can be seen below as the training loss is 4770405.5 and accuracy is 0.101. On some occasions, the loss will converge back after suddenly increasing. We found that if we are using a high number of iterations this phenomenon can repeat multiple times. \n",
        "\n",
        "Betas|Learning Rate|Batch Size|Epochs|Training Loss|Validation Accuracy\n",
        "---|---|---|---|---|---|\n",
        "(0.9,0.999)|0.001|10|50|0.095|0.665\n",
        "(0.9, 0.999)|0.001|10|200|0.013|0.822\n",
        "(0.9, 0.999)|0.001|10|400|0.139|0.463\n",
        "(0.9, 0.999)|0.001|10|500|4770405.5|0.101\n",
        "\n",
        "\n",
        "When using Nesterov momentum, we observed it produced bad results as it caused the training loss to fluctuate a lot. This can be seen when we also observed that if the loss was above 0.1, it would result in low accuracy regardless of the hyperparameters the optimizer was using. If the loss was below 0.04, we found that the model was overfitting to the training data. \n",
        "\n",
        "Betas|Learning Rate|Batch Size|Epochs|Nesterov Momentum|Weight Decay|Training Loss|Validation Accuracy\n",
        "---|---|---|---|---|---| --- | --- |\n",
        "(0.9, 0.99)|0.001|10|100|Yes|0.004|0.433|0.005\n",
        "(0.9, 0.99)|0.001|10|200|Yes|0.004|NaN|---\n",
        "(0.9, 0.99)|0.001|10|400|Yes|0.004|NaN|---\n",
        "\n",
        "\n",
        "In comparison with other optimizers, we found Adam performed the best with the top result being an accuracy of 0.857 and training loss of 0.008. Therefore, we decided to use Adam as the optimizer for our model. \n",
        "\n",
        "###Adam with augmented data:\n",
        "From our earlier experiments, we concluded that Adam outperformed the other optimizers. The results below show results from experiments where we are determining the best hyperparameters for the optimizer functions when using augmented data.\n",
        "\n",
        "Due to the increased dataset for training we were able to increase our batch size to 25, which resulted in a ~0.05 increase in accuracy as the gradients being calculated are more accurate. A higher batch size also meant that we needed to run fewer epochs in order to get low loss and high validation accuracy. The data augmentation has been done in 2 batches and after the second augmentation we noticed some spikes in the running loss while training the model. We decided to reduce the learning rate in order to mitigate the issue and stabilize the model. We already knew that a lower training loss might be a sign of overfitting, but we still decided to test a higher number of epochs, and our expectations have been met, validation loss beginning to go down after 60 epochs. We have done a lot more testing than shown below, and tested each set of hyperparameters multiple times, but we decided to only include the most relevant data in the report to avoid clutter.\n",
        "\n",
        "Overall we found that a learning rate of 0.0002 and 50 epochs gave us the results with a validation accuracy of 0.885.\n",
        "\n",
        " Betas|Learning Rate|Batch Size|Epochs|Training Loss|Validation Accuracy\n",
        "---|---|---|---|---|---| \n",
        "(0.9, 0.99)|0.0005|25|10|0.320|0.650\n",
        "(0.9, 0.99)|0.0005|25|20|0.127|0.861\n",
        "(0.9, 0.99)|0.0005|25|30|0.088|0.840\n",
        "(0.9, 0.99)|0.0005|25|40|0.069|0.850\n",
        "(0.9, 0.99)|0.0005|25|50|0.055|0.863\n",
        "(0.9, 0.99)|0.0005|25|60|0.034|0.882\n",
        "(0.9, 0.99)|0.0003|25|30|0.066|0.883\n",
        "(0.9, 0.99)|0.0002|25|10|0.115|0.807\n",
        "(0.9, 0.99)|0.0002|25|20|0.076|0.874\n",
        "(0.9, 0.99)|0.0002|25|30|0.060|0.878\n",
        "(0.9, 0.99)|0.0002|25|40|0.050|0.881\n",
        "(0.9, 0.99)|0.0002|25|50|0.041|0.885\n",
        "(0.9, 0.99)|0.0002|25|60|0.034|0.882\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uaQJF1Wr1KnD"
      },
      "source": [
        "#Conclusion\n",
        "\n",
        "Through this coursework we have reached the conclusion that the U-Net model, along with weighted cross entropy and Adam optimizer gave the best results. Dice Loss was used to evaluate the validation set. Using a learning rate of 0.0002, batch size of 25 images, 50 epochs and weights of [0.1,0.3,0.3,0.3] gave the best overall training and validation score. The biggest challenge posed was the lack of adequate datasets which is a common problem for AI in medical fields as obtaining a lot of data is an expensive and time-consuming process. We have adopted data augmentation techniques to create more training images to enable the model to learn better and overcome this issue.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "auUf-izb4mr_"
      },
      "source": [
        "#References\n",
        "\n",
        "1: https://www.creatis.insa-lyon.fr/Challenge/acdc/\n",
        "\n",
        "2: https://arxiv.org/abs/1505.04597.pdf\n",
        "\n",
        "3: https://arxiv.org/pdf/1606.00915.pdf\n",
        "\n",
        "4: https://arxiv.org/pdf/1511.00561.pdf\n",
        "\n",
        "5: https://arxiv.org/abs/1705.08790.pdf\n",
        "\n",
        "6: https://en.wikipedia.org/wiki/S%C3%B8rensen%E2%80%93Dice_coefficient\n",
        "\n",
        "7: https://arxiv.org/pdf/1708.02002.pdf\n",
        "\n",
        "8: https://arxiv.org/pdf/1812.07032.pdf\n",
        "\n",
        "9: https://machinelearningmastery.com/cross-entropy-for-machine-learning/\n",
        "\n",
        "10: https://towardsdatascience.com/optimizers-for-training-neural-network-59450d71caf6 \n",
        "\n",
        "11: https://machinelearningmastery.com/adam-optimization-algorithm-for-deep-learning/"
      ]
    }
  ]
}